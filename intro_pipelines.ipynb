{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines: Or How I Learned to Stop Duplicating Work and Love the Convenience of Storing Multiple Steps in a Single Object\n",
    "\n",
    "Original author: [Cristian E. Nuno](https://github.com/cenuno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from typing import List, Optional\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics.classification import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load necessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load iris data set\n",
    "iris = load_iris()\n",
    "\n",
    "# load feature matrix\n",
    "X = iris[\"data\"]\n",
    "# load target vector\n",
    "y = iris[\"target\"]\n",
    "\n",
    "# standardize feature names spelling and casing\n",
    "feature_names = [col.replace(\" \", \"_\").replace(\"_(cm)\", \"\") \n",
    "                 for col in (iris[\"feature_names\"] + [\"species\"])]\n",
    "\n",
    "# transform X and y into data frame\n",
    "iris_df = pd.DataFrame(np.column_stack((X, y.reshape(-1, 1)))\n",
    "                       , columns=feature_names)\n",
    "\n",
    "# convert species from numeric to categorical\n",
    "class_names = {0.0: \"setosa\", 1.0: \"versicolor\", 2.0: \"virginica\"}\n",
    "iris_df[\"species\"] = iris_df[\"species\"].map(class_names)\n",
    "\n",
    "# show first few records\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For educational purposes only, let's replace some values with `NaN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize random number generator\n",
    "random.seed(2019)\n",
    "\n",
    "# generate list of random integers \n",
    "random_ints = [random.randrange(0, len(iris_df)) for _ in range(20)]\n",
    "\n",
    "# for these random index values, replace their real values with NaN\n",
    "iris_df.loc[random_ints, \"sepal_length\"] = np.nan\n",
    "iris_df.loc[random_ints, \"sepal_width\"] = np.nan\n",
    "\n",
    "# manually force a few \"sepal_width\" value to also be NaN\n",
    "iris_df.loc[0:25, \"sepal_width\"] = np.nan\n",
    "\n",
    "iris_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split `iris_df` into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(iris_df.drop(\"species\", axis=1),\n",
    "                                                    iris_df[\"species\"],\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for modeling by cleaning it\n",
    "\n",
    "1. Flag which records contain `NaN` values in the `sepal_length` and `sepal_width` features;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_missing(x: float) -> int:\n",
    "    \"\"\"Identifies if an element is missing\n",
    "    Args:\n",
    "        - x (float): element from a series\n",
    "    Returns:\n",
    "        int: 1 if the element is missing; 0 otherwise\n",
    "    \"\"\"\n",
    "    if np.isnan(x):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"sepal_length_missing\"] = X_train[\"sepal_length\"].apply(is_missing)\n",
    "X_train[\"sepal_width_missing\"] = X_train[\"sepal_width\"].apply(is_missing)\n",
    "X_train.head(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Impute the median value for `NaN` values in the `sepal_length` and `sepal_width` features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Median value for sepal length from X_train: {X_train['sepal_length'].median()}\")\n",
    "print(f\"Median value for sepal width from X_train: {X_train['sepal_width'].median()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imp = SimpleImputer(missing_values=np.nan, strategy=\"median\")\n",
    "imp.fit(X_train)\n",
    "X_train_clean = pd.DataFrame(data=imp.transform(X_train),\n",
    "                             columns=X_train.columns,\n",
    "                             index=X_train.index)\n",
    "X_train_clean.head(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from above, we've already satisified the 3rd piece of logic, which was:\n",
    "\n",
    "3. Return all other columns - `petal_length` & `petal_width` - as is.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's fit our data to a model and see how it does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build classifier\n",
    "dt_clf = DecisionTreeClassifier(random_state=2019,\n",
    "                                criterion=\"gini\")\n",
    "\n",
    "dt_clf.fit(X_train_clean, y_train)\n",
    "\n",
    "print(classification_report(y_test, dt_clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happened?\n",
    "\n",
    "Calling `dt_clf.predict()` requires that `X_test` be cleaned/processed in the same as the data that `dt_clf` was fit on (e.g. `X_train_clean`).\n",
    "\n",
    "#### Brute force\n",
    "\n",
    "One way to resolve this is by copying and pasting our logic and swapping out `X_train_clean` for `X_test`\n",
    "\n",
    "```python\n",
    "# flag missing values\n",
    "X_test[\"sepal_length_missing\"] = X_test[\"sepal_length\"].apply(is_missing)\n",
    "X_test[\"sepal_width_missing\"] = X_test[\"sepal_width\"].apply(is_missing)\n",
    "\n",
    "# impute the median for missing values\n",
    "X_test_clean = pd.DataFrame(data=imp.transform(X_test),\n",
    "                             columns=X_test.columns,\n",
    "                             index=X_test.index)\n",
    "\n",
    "# see how well the model does\n",
    "print(classification_report(y_test, dt_clf.predict(X_test_clean)))\n",
    "```\n",
    "\n",
    "The flaw with this strategy is that your code starts to have repetition in that you are duplicating logic (a violation of the DRY principle).\n",
    "\n",
    "#### Instead, wrap your logic into smaller functions\n",
    "\n",
    "By breaking the cleaning steps into smaller functions, you make it easier for your future self to debug and maintain existing code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_missing_flags(df: pd.DataFrame, col_names: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Create a new column that flags which elements are missing\n",
    "    Args:\n",
    "        - df: train/test data frame\n",
    "        - col_names: column name\n",
    "    Returns:\n",
    "        Data frame with the newly added missing flag columns\n",
    "    \"\"\"\n",
    "    for col_name in col_names:\n",
    "        df[f\"{col_name}_missing\"] = df[f\"{col_name}\"].apply(is_missing)\n",
    "        \n",
    "    return df\n",
    "    \n",
    "def impute_missing_values(df: pd.DataFrame, imp: SimpleImputer) -> pd.DataFrame:\n",
    "    \"\"\"Impute missing values with the median for a data frame\n",
    "    Args:\n",
    "        - df:  a data frame\n",
    "        - imp: a SimpleImputer object\n",
    "    Returns:\n",
    "        Data frame with imputed values for those that were missing\n",
    "    \"\"\"\n",
    "    imputed_df = pd.DataFrame(data=imp.transform(df),\n",
    "                              columns=df.columns,\n",
    "                              index=df.index)\n",
    "    return imputed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Once your smaller functions are done, you can then place them into one larger function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(train_df: pd.DataFrame,\n",
    "               col_names: List[str],\n",
    "               test_df: Optional[pd.DataFrame] = None) -> pd.DataFrame:\n",
    "    \"\"\"Cleans either the train or test set\n",
    "    Args:\n",
    "        - train_df:  training data frame\n",
    "        - col_names: list of column names that contain missing values\n",
    "        - test_df:   testing data frame\n",
    "    Return:\n",
    "        if train_df is supplied but not test_df, cleaned train_df will be returned;\n",
    "        if both train and test df are supplied, cleaned test_df will be returned\n",
    "    \"\"\"\n",
    "    # create an imputer object\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy=\"median\")\n",
    "    # fit the imputer to the training set\n",
    "    _ = imp.fit(train_df)\n",
    "    \n",
    "    \n",
    "    # make copy of the input df\n",
    "    if test_df is None:\n",
    "        copy_df = train_df.copy()\n",
    "    elif test_df is not None:\n",
    "        copy_df = test_df.copy()\n",
    "    \n",
    "    # flag missing values\n",
    "    clean_df = make_missing_flags(df=copy_df,\n",
    "                                  col_names=col_names)\n",
    "    \n",
    "    # replace missing values with the median\n",
    "    clean_df = impute_missing_values(df=clean_df,\n",
    "                                     imp=imp)\n",
    "    \n",
    "    return clean_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's use our preprocessor  function to clean `X_test` and then retry evaluating our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_clean = clean_data(train_df=X_train,\n",
    "                          col_names=[\"sepal_length\", \"sepal_width\"],\n",
    "                          test_df=X_test)\n",
    "\n",
    "print(classification_report(y_test, dt_clf.predict(X_test_clean)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing `sklearn.pipeline.Pipeline()` object\n",
    "\n",
    "![kid transformer](visuals/transformer.gif)\n",
    "\n",
    "### Definition of a pipeline\n",
    "\n",
    "We'll store these steps in a `Pipeline` object. From the [docs](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn-pipeline-pipeline):\n",
    "\n",
    "> The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. [The `Pipeline` object] sequentially applies a list of transforms and a final estimator. Intermediate steps of the pipeline must be ‘transforms’ (i.e. transformers), that is, they must implement fit and transform methods. \n",
    "\n",
    "### Broad generalization of a pipeline\n",
    "\n",
    "The key here is we need to specify a specific column, pass it's \"transformer\" (i.e. `SimpleImputer`, `OneHotEncoder`, `StandardScaler`), and determine if the transformation belongs in it's own new column or if it's more appropriate for the transformed column to overwrite the input column.\n",
    "\n",
    "### Benefits of the `Pipeline`\n",
    "\n",
    "From the [User Guide](https://scikit-learn.org/stable/modules/compose.html#pipeline-chaining-estimators):\n",
    "> * **Convenience and encapsulation**\n",
    ">     + You only have to call fit and predict once on your data to fit a whole sequence of estimators.\n",
    "> * **Joint parameter selection**\n",
    ">     + You can grid search over parameters of all estimators in the pipeline at once.\n",
    "> * **Safety**\n",
    ">     + Pipelines help avoid leaking statistics from your test data into the trained model in cross-validation, by ensuring that the same samples are used to train the transformers and predictors. \n",
    "\n",
    "Here, we'll create a few `Pipeline` objects to do the following:\n",
    "\n",
    "* flag which records contain `NaN` values in the `sepal_length` and `sepal_width` features;\n",
    "* impute the median value for `NaN` values in the `sepal_length` and `sepal_width` features; and\n",
    "* return all other columns - `petal_length` & `petal_width` - as is\n",
    "\n",
    "### Create custom transformer that identifies records that have `NaN` values\n",
    "\n",
    "Sometimes we need to add new features to our existing feature space. In this case, we can't rely on importing a traditional transformer (i.e. `StandardScaler`, `OneHotEncoder`, etc.).\n",
    "\n",
    "Instead, we'll need to create our own custom transformer. We will do this by creating a new class that implements both `.fit()` and `.transform()` methods. \n",
    "\n",
    "*Shoutout to [Sebastian Raschka](https://sebastianraschka.com/) for helping me out on [Twitter](https://twitter.com/cenuno_/status/1179855832374099968) to figure this out!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsMissing(BaseEstimator):\n",
    "    \"\"\"Creates a new column flagging if any values from one column are missing\n",
    "    \n",
    "    Note: this class will be used inside a scikit-learn Pipeline\n",
    "    \n",
    "    Attributes:\n",
    "        col_name (str): name of a column\n",
    "        \n",
    "    Methods:\n",
    "        _is_missing(): returns 1 if record contains NaN value; 0 if else\n",
    "        \n",
    "        fit(): fit all the transformers one after the other \n",
    "               then fit the transformed data using the final estimator\n",
    "               \n",
    "        transform(): apply transformers, and transform with the final estimator\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, col_name):\n",
    "        self.col_name = col_name\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def _is_missing(self, X):\n",
    "        \"\"\"Flag if a record has a NaN value\"\"\"\n",
    "        if pd.isna(X):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"Copies X and creates a new column before returning X_new\"\"\"\n",
    "        new_col = self.col_name + \"_missing\"\n",
    "        X_new = X.copy()\n",
    "        X_new[new_col] = X_new[self.col_name].apply(self._is_missing)\n",
    "        return X_new\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create first `Pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_mapper = Pipeline(steps=[\n",
    "    (\"missing_sl\", IsMissing(col_name=\"sepal_length\")),\n",
    "    (\"missing_sw\", IsMissing(col_name=\"sepal_width\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_mapper.fit(X_train).transform(X_train).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a pipeline to impute the median values for `sepal_length` and `sepal_width`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Median value for sepal length from X_train: {X_train['sepal_length'].median()}\")\n",
    "print(f\"Median value for sepal width from X_trani: {X_train['sepal_width'].median()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_mapper = ColumnTransformer(transformers=[\n",
    "    (\"impute\", SimpleImputer(missing_values=np.nan, strategy=\"median\"), ['sepal_length', 'sepal_width'])\n",
    "],\n",
    "                      remainder=\"passthrough\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: we're setting `remainder` to \"passthrough\" so that all non `sepal_length` and `sepal_width` columns are returned without any transformations_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_mapper.fit(X_train).transform(X_train)[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above, notice the 14 and 15th records: they were previously `NaN`. After using the `SimpleImputer` transformer, the `NaN` values were replaced with the median values of `sepal_length` and `sepal_width`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the two pipelines into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_mapper = Pipeline(steps=[\n",
    "    (\"missing\", missing_mapper),\n",
    "    (\"impute\", impute_mapper)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_mapper.fit(X_train).transform(X_train)[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's take this up by perfoming all the preprocessing steps prior to building a `DecisionTreeClassifier` model\n",
    "\n",
    "This model will help us classify which species a flower is from `X_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build classifier\n",
    "dt_clf = DecisionTreeClassifier(random_state=2019,\n",
    "                                min_samples_leaf=30,\n",
    "                                criterion=\"gini\",\n",
    "                                min_samples_split=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build pipeline\n",
    "pipe = Pipeline(steps=[\n",
    "    (\"dataprep\", data_prep_mapper),\n",
    "    (\"model\", dt_clf)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit `X_train` and `y_train` onto the `pipe` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `pipe` object to make predictions on `X_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You now know how to take advantage of `Pipeline` objects to stop duplicating your preprocessing steps in the training and testing sets. I hope this helps you take advantage of yet another module built into `scikit-learn` to help improve your machine learning workflow!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (prework-labs)",
   "language": "python",
   "name": "prework-labs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
